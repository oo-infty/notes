type:: note
tags:: Artificial Intelligence, Machine Learning, Supervised Learning, Classification

- **基本思想**
	- 直观上看，如果一个样本距离划分决策边界的超平面很远，那么对其预测的结果就很可能是正确的。
	- SVM 试图找到一个最优的超平面，使得各样本距离这个超平面尽可能远。
- **前置定义**
	- 在大小为 $m$ 的样本集 $S$ 中，一个样本是 $\bm x^{(i)}\in \mathrm R^n$，对应标签 $y^{(i)}\in\{1, -1\}$。
	- SVM 求解的超平面表示为 $\bm w^{\mathrm T}\bm x+b=0$。
	- 设 $g(x)=\left\{\begin{matrix}1,& x\ge 0\\-1,& x<0\\\end{matrix}\right.$
- **函数间隔和几何间隔**
	- $(\bm x^{(i)},y^{(i)})$ 的函数间隔表示为 $\hat{\gamma}^{(i)}=y^{(i)}(\bm w^{\mathrm T}\bm x^{(i)}+b)$，$S$ 的函数间隔表示为 $\hat{\gamma}=\min\hat{\gamma}^{(i)}$。
		- 需要注意函数间隔的大小是相对的，如果等比例缩放 $\bm w$ 和 $b$，$\hat{\gamma}$ 也会等比例缩放，而每个样本的决策不改变。
		- 一种限制这种情况的方法是添加 $\|\bm w\|=1$。
	- $(\bm x^{(i)},y^{(i)})$ 的几何间隔表示为 $\gamma^{(i)}=\frac{y^{(i)}(\bm w^{\mathrm T}\bm x^{(i)}+b)}{\|\bm w\|}$，$S$ 的几何间隔表示为 $\gamma=\min\gamma^{(i)}$。
		- $\|\bm w\|=1$ 情况下，函数间隔等于几何间隔。
- **最优间隔分类器**
	- **定义**
		- 在样本线性可分情况下，最优间隔分类器可以找到一个最优的超平面，使得样本的几何间隔或 $\|\bm w\|=1$ 的函数间隔最大。
		- 最优间隔分类器是 SVM 的一个核心组成部分。
	- **最优化问题**
		- 最优化问题可以表示为
		  $$
		  \max\gamma\ (\text{s.t. } \forall i,y^{(i)}(\bm w^{\mathrm T}\bm x^{(i)}+b) \ge \gamma,\|\bm w\|=1)
		  $$
		- 其中不等式的限制可以两边同除以 $\gamma$，$y^{(i)}\left(\frac{\bm w^{\mathrm T}}{\gamma}\bm x^{(i)}+\frac{b}{\gamma}\right) \ge 1$。
		- 将 $\frac{\bm w^{\mathrm T}}{\gamma},\frac{b}{\gamma}$ 替换为 $\bm w',b'$，则 $\gamma=\frac{\|\bm w\|}{\|\bm w'\|}=\frac{1}{\|w'\|}$，则最优化问题可以表示为
		  $$
		  \max\frac{1}{\|\bm w'\|}\ (\text{s.t. } \forall i,y^{(i)}(\bm w'^{\mathrm T}\bm x^{(i)}+b') \ge 1)
		  $$
		- 可以看到 $\bm w$ 和 $\|\bm w\|=1$ 已经被去掉，可以改写为可以进行凸优化的最终形式
		  $$
		  \min \frac{1}{2}\|\bm w\|^2\ (\text{s.t. } \forall i,y^{(i)}(\bm w^{\mathrm T}\bm x^{(i)}+b) \ge 1)
		  $$
		- 设
		  $$
		  \mathcal L(\bm w, b,\bm \alpha)=\frac{1}{2}\|\bm w\|^2+\sum_{i=1}^{m}\alpha_i [1 - y^{(i)}(\bm w^{\mathrm T}\bm x^{(i)}+b)]
		  $$
		- 运用 KKT 条件：
		  $$
		  \frac{\partial\mathcal L}{\partial w_i} =0, i=1\dots m\\
		  \frac{\partial\mathcal L}{\partial b} =0\\
		  1 - y^{(i)}(\bm w^{\mathrm T}\bm x^{(i)}+b)\le 0, i=1\dots m\\
		  \alpha_i\ge 0, i=1\dots m\\
		  \alpha_i [1 - y^{(i)}(\bm w^{\mathrm T}\bm x^{(i)}+b)]=0, i=1\dots m\\
		  $$
		- 其中 $\frac{\partial\mathcal L}{\partial w_i} =0, i=1\dots m$ 推出
		  $$
		  \bm w^*=\sum_{i=1}^m \alpha_i y^{(i)} x^{(i)}
		  $$
		  $\frac{\partial\mathcal L}{\partial b}=0$ 推出
		  $$
		  b^*=-\sum_{i=1}^m \alpha_i y^{(i)}
		  $$
		- 代回后得到
		  $$
		  \mathcal L(\bm w^*, b^*, \alpha)=\sum_{i=1}^m \alpha_i - \sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y^{(i)} y^{(j)} \langle x^{(i)}, x^{(j)} \rangle
		  $$
		- 以上是原始优化问题，则对偶优化问题为
		  $$
		  \max_{\bm \alpha}\ W(\bm \alpha)=\max_{\alpha} \left(\sum_{i=1}^m \alpha_i - \sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y^{(i)} y^{(j)} \langle x^{(i)}, x^{(j)} \rangle \right) \left(\text{s.t. }\forall i,\alpha_i \ge 0, \sum_{i=1}^m \alpha_i y^{(i)} = 0\right)
		  $$
		- 可以证明原始优化问题与对偶优化问题求解的值相同。
		- 求解 $\max\limits_{\bm \alpha}\ W(\bm \alpha)$ 使用 SMO 算法。
	- **预测**
		- 求出 $\bm\alpha$ 后，可以求出 $\bm w$ 和 $b$。
		- 假设函数定义为 $h_{\bm w,b}(\bm x)=g(\bm w^{\mathrm T}\bm x+b)$。
- **核方法和 SVM**
	- **定义**
		- SVM 相比最优间隔分类器的改进是核方法。
		- 对于线性不可分的数据，一种方法是将 $\bm x$ 映射到更高维的向量，添加更多的特征。
		- 比如 $\bm x\in \mathrm R^3$，则可以添加二次的特征，通过以下映射实现：
		  $$
		  \phi:\begin{bmatrix}x_1 \\x_2 \\x_3\end{bmatrix} \longmapsto
		  \begin{bmatrix}x_1x_1 \\x_1x_2 \\x_1x_3 \\x_2x_1 \\x_2x_2 \\x_2x_3 \\x_3x_1 \\x_3x_2 \\x_3x_3 \\\end{bmatrix}
		  $$
		- 核函数 $K(\bm x,\bm z)$ 可以快速计算 $\langle \phi(\bm x),\phi(\bm z) \rangle$，即使计算 $\phi(\bm x)$ 很困难或不可能，如映射到无限维向量。
		- 在以上的例子中，$K(\bm x,\bm z)=(\bm x^{\mathrm T}\bm z)^2$。
	- **应用**
		- 可以发现最优间隔分类器中涉及到 $\bm x$ 的部分都是以内积形式出现，所以将 $\bm x$ 替换为 $\phi(\bm x)$ 后，$\langle \phi(\bm x),\phi(\bm z) \rangle$ 可以换成 $K(\bm x,\bm z)$。
		- $$
		  W(\alpha)=\sum_{i=1}^m \alpha_i - \sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y^{(i)} y^{(j)} K(x^{(i)}, x^{(j)})\\
		  h_{\bm w,b}(\bm x)=g\left(\sum_{i=1}^m \alpha_i y^{(i)} K(\bm x^{(i)}, \bm x) + b\right)\\
		  $$
	- **常用的核函数**
		- 内积核（不对 $\bm x$ 进行映射）：$K(\bm x, \bm z)=\bm x^{\mathrm T}\bm z$
		- 多项式核（$\bm x$ 的分量任意组合，得到各种次数，最高次为 $p$）：$K(\bm x,\bm z)=(\bm x^T\bm z+c)^p$
		- 高斯核 / 径向基函数（无限次数）：$K(\bm x,\bm z)=\exp\left(-\frac{\|\bm x-\bm z\|^2}{\sigma^2}\right)$