type:: note
tags:: Artificial Intelligence, Machine Learning, Regression

- **定义**
	- $\bm x\in\mathrm R^n$ 是一个有 $n$ 个特征的向量，以 $\bm\theta\in\mathrm R^{n+1}$ 为参数，定义假设函数 $h_{\bm\theta}(\bm x)=\theta_1x_1+\theta_2x_2\cdots+\theta_nx_n+\theta_0$。
	- 使用 $h_{\bm\theta}(\bm x)$ 进行回归，这就是线性回归。
	- 为了方便，我们把 $\bm x$ 改为 $n+1$ 元向量，规定 $x_0=1$，则 $h_{\bm\theta}(\bm x)=\bm\theta^{\mathrm T}\bm x$。
- **训练**
	- **梯度下降**
		- **前置定义**
			- 定义损失函数 $J(\bm\theta)=\frac{1}{2}\sum\limits_{i=1}^m (h_{\bm\theta}(\bm x^{(i)})-y^{(i)})^2$，表示训练样本和假设的预测的偏差程度。
			- 训练的过程就是寻找 $J(\bm\theta)$ 的最小值点。
			- $J(\bm\theta)$ 是一个凸函数，其只有一个极小值点，并且是全局的最小值点，可以使用梯度下降的方法求解。
		- **方法**
			- 初始时，给 $\bm\theta$ 赋任意值，初始化学习率 $\alpha$ 和 $\varepsilon$。
			- 不断迭代更新 $\bm\theta$：
				- 令 $\bm\theta$ 向 $J(\bm\theta)$ 的梯度的反方向移动，使 $J(\bm\theta)$ 减小，即对于 $j=0,1,\dots,n$：
				  $$
				  \begin{aligned}
				  \theta_j &\leftarrow \theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\bm\theta)\\
				  &=\theta_j-\alpha\sum_{i=1}^m (\bm\theta^{\mathrm T}\bm x^{(i)}-y^{(i)})x_j^{(i)}
				  \end{aligned}
				  $$
				- 设 $X=\begin{bmatrix} (\bm x^{(1)})^{\mathrm T} \\(\bm x^{(2)})^{\mathrm T} \\\vdots \\(\bm x^{(m)})^{\mathrm T}\end{bmatrix}$ 为所有训练样本，$\bm y=\begin{bmatrix}y^{(1)} \\y^{(2)} \\\vdots \\y^{(m)}\end{bmatrix}$ 为对应的标签，对于 $\bm\theta$ 整体：
				  $$
				  \begin{aligned}
				  \bm\theta &\leftarrow \bm\theta-\alpha\nabla_{\bm\theta} J(\bm\theta)\\
				  &=\bm\theta-\alpha\sum_{i=1}^m(\bm\theta^{\mathrm T}\bm x^{(i)}-y^{(i)})\bm x^{(i)}\\
				  &=\bm\theta-\alpha X^{\mathrm T}(X\bm\theta-\bm y)
				  \end{aligned}
				  $$
				- 当 $\|\Delta\bm\theta\|<\varepsilon$ 时，可以判断 $\bm\theta$ 已经收敛，终止迭代。
		- **学习率的选取**
			- 学习率 $\alpha$ 决定了每一次迭代 $\bm\theta$ 改变程度有多大。
			- $\alpha$ 的大小对训练的影响：
				- $\alpha$ 偏大：改变 $\bm\theta$ 导致跨越最小值点，无法收敛，可能表现为 $J(\bm\theta)$ 不断增大。
				- $\alpha$ 偏小：$\bm\theta$ 改变程度太小，收敛速度太慢。