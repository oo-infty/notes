---
tags:
  - artificial-intelligence
  - machine-learning
  - regression
---

- **定义**
	- $\bm x\in\mathrm R^n$ 是一个有 $n$ 个特征的向量，以 $\bm\theta\in\mathrm R^{n+1}$ 为参数，定义假设函数 $h_{\bm\theta}(\bm x)=\theta_1x_1+\theta_2x_2\cdots+\theta_nx_n+\theta_0$。
	- 使用 $h_{\bm\theta}(\bm x)$ 进行回归，这就是线性回归。
	- 为了方便，我们把 $\bm x$ 改为 $n+1$ 元向量，规定 $x_0=1$，则 $h_{\bm\theta}(\bm x)=\bm\theta^{\mathrm T}\bm x$。
- **训练**
	- **批量梯度下降**
		- **前置定义**
			- 定义损失函数 $J(\bm\theta)=\frac{1}{2}\sum\limits_{i=1}^m (h_{\bm\theta}(\bm x^{(i)})-y^{(i)})^2$，表示训练样本和假设的预测的偏差程度。
			- 训练的过程就是寻找 $J(\bm\theta)$ 的最小值点。
			- $J(\bm\theta)$ 是一个凸函数，其只有一个极小值点，并且是全局的最小值点，可以使用梯度下降的方法求解。
		- **方法**
			- 初始时，给 $\bm\theta$ 赋任意值，初始化学习率 $\alpha$ 和 $\varepsilon$。
			- 不断迭代更新 $\bm\theta$：
				- 令 $\bm\theta$ 向 $J(\bm\theta)$ 的梯度的反方向移动，使 $J(\bm\theta)$ 减小，即对于 $j=0,1,\dots,n$：
				  $$
				  \begin{aligned}
				  \theta_j &\leftarrow \theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\bm\theta)\\
				  &=\theta_j-\alpha\sum_{i=1}^m (\bm\theta^{\mathrm T}\bm x^{(i)}-y^{(i)})x_j^{(i)}
				  \end{aligned}
				  $$
				- 设 $X=\begin{bmatrix} (\bm x^{(1)})^{\mathrm T} \\(\bm x^{(2)})^{\mathrm T} \\\vdots \\(\bm x^{(m)})^{\mathrm T}\end{bmatrix}$ 为所有训练样本，$\bm y=\begin{bmatrix}y^{(1)} \\y^{(2)} \\\vdots \\y^{(m)}\end{bmatrix}$ 为对应的标签，对于 $\bm\theta$ 整体：
				  $$
				  \begin{aligned}
				  \bm\theta &\leftarrow \bm\theta-\alpha\nabla_{\bm\theta} J(\bm\theta)\\
				  &=\bm\theta-\alpha\sum_{i=1}^m(\bm\theta^{\mathrm T}\bm x^{(i)}-y^{(i)})\bm x^{(i)}\\
				  &=\bm\theta-\alpha X^{\mathrm T}(X\bm\theta-\bm y)
				  \end{aligned}
				  $$
				- 当 $\|\Delta\bm\theta\|<\varepsilon$ 时，可以判断 $\bm\theta$ 已经收敛，终止迭代。
		- **学习率的选取**
			- 学习率 $\alpha$ 决定了每一次迭代 $\bm\theta$ 改变程度有多大。
			- $\alpha$ 的大小对训练的影响：
				- $\alpha$ 偏大：改变 $\bm\theta$ 导致跨越最小值点，无法收敛，可能表现为 $J(\bm\theta)$ 不断增大。
				- $\alpha$ 偏小：$\bm\theta$ 改变程度太小，收敛速度太慢。
	- **随机梯度下降**
		- **方法**
			- 与批量梯度下降类似，但是在迭代更新 $\bm\theta$ 的过程中有所区别。
			- 随机梯度下降每次迭代仅计算一个训练样本的梯度，即
			  $$
			  \bm\theta \leftarrow \bm\theta_j-\alpha (\bm\theta^{\mathrm T}\bm x^{(i)}-y^{(i)})\bm x^{(i)}
			  $$
			- 如果画出随机梯度下降的 $\bm\theta$ 的路径，则其会以不规则的折线向最小值点移动，最终在最小值点附近随机振荡。
		- **比较**
			- 随机梯度下降训练速度快，每次更新 $\theta$ 不需要完全遍历数据。
			- 随机梯度下降在面对大量训练数据时表现与批量梯度下降接近。
			- 随机梯度下降没有一个明显的收敛的状态，一般要额外限制迭代次数。
	- **正规方程组 / 最小二乘法**
		- **定义**
			- 理想情况下，$\bm\theta$ 满足 $X\bm\theta=\bm y$，但实际上这个方程组是无解的。
			- 训练只要找到满足 $\| X\bm\theta -\bm y\|$ 最小的一个 $\bm\theta$ 即可，这个 $\bm\theta$ 就是 $X\bm\theta=\bm y$ 的最小二乘解。
			- $J(\bm\theta)=\frac{1}{2}(X\bm\theta -\bm y)^{\mathrm T}(X\bm\theta -\bm y)$
		- **方法**
			- 正规方程组为 $X^{\mathrm T}X\bm\theta=X^{\mathrm T}\bm y$。
			- 解出 $\bm\theta=(X^{\mathrm T}X)^{-1}X^{\mathrm T}\bm y$。
		- **比较**
			- 解正规方程组可以直接得出最优的 $\bm\theta$，而不用像梯度下降算法一样迭代。
			- 面对大量训练数据时解正规方程组很慢，不如随机梯度下降。
			- 梯度下降可以用在其他算法中，应用更广泛。
- **预测**
	- 根据 $h_{\bm\theta}(\bm x)=\bm\theta^{\mathrm T}\bm x$ 直接计算即可。
- **局部加权线性回归**
	- **定义**
		- 对于无法线性回归的数据，通常可以根据待预测的 $\bm x$，选择 $\bm x$ 附近的训练样本进行局部的线性回归，再进行预测。
		- 修改损失函数为
		  $$
		  J(\bm\theta)=\frac{1}{2}\sum\limits_{i=1}^m w^{(i)}(h_{\bm\theta}(\bm x^{(i)})-y^{(i)})^2
		  $$
		  其中 $w^{(i)}$ 代表权重，$\bm x^{(i)}$ 距离 $\bm x$ 越近，则 $w^{(i)}$ 越大。
		- 一种可能的 $w^{(i)}$ 表达式：
		  $$
		  w^{(i)}=\exp\left(-\frac{\| \bm x-\bm x^{(i)}\|^2}{2\tau^2}\right)
		  $$
	- **方法**
		- 对于每个预测数据，在线重新训练。
		- 梯度下降更新 $\bm\theta$ 的过程修改为
		  $$
		  \theta_j \leftarrow \theta_j-\alpha\sum_{i=1}^m w^{(i)}(\bm\theta^{\mathrm T}\bm x^{(i)}-y^{(i)})x_j^{(i)}
		  $$
		- 设 $W=\begin{bmatrix}w^{(1)} & & &\\& w^{(2)} & &\\& & \ddots & \\& & & w^{(m)}\end{bmatrix}$，则
		  $$
		  \begin{aligned}
		  \bm\theta &\leftarrow \bm\theta-\alpha\nabla_{\bm\theta} J(\bm\theta)\\
		  &=\bm\theta-\alpha\sum_{i=1}^m w^{(i)}(\bm\theta^{\mathrm T}\bm x^{(i)}-y^{(i)})\bm x^{(i)}\\
		  &=\bm\theta-\alpha X^{\mathrm T}W(X\bm\theta-\bm y)
		  \end{aligned}
		  $$
		- 正规方程组修改为 $X^{\mathrm T}WX\bm\theta=X^{\mathrm T}W\bm y$。